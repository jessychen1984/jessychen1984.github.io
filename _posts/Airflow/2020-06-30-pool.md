---
title: "Airflow Poolé™åˆ¶taskså¹¶å‘"
subtitle: "Using pool to limit tasks concurrency in Airflow"
layout: post
author: "jessychen"
header-style: text
tags:
  - Airflow
  - CS
---

> Some systems can get overwhelmed when too many processes hit them at the same time. Airflow pools can be used to **limit the execution parallelism** on arbitrary sets of tasks.

* [èƒŒæ™¯](#èƒŒæ™¯)
* [å®ç°](#å®ç°)
* [æºç ](#æºç )
* [æ€»ç»“](#æ€»ç»“)

### èƒŒæ™¯

æœ€è¿‘ï¼Œé¡¹ç›®ä¸­ç”¨Airflowé‡å†™äº†æŸä¸ªç³»ç»Ÿçš„è°ƒåº¦æ–¹å¼ã€‚æœ¬åœ°å„ç§æµ‹è¯•éƒ½å¥½å¥½çš„ï¼Œè€Œæ”¾åˆ°çº¿ä¸Šåï¼Œçªç„¶æ”¶åˆ°å¤§é‡task failçš„æŠ¥è­¦:dizzy_face:ã€‚æŸ¥çœ‹logå‘ç°å‡ ä¸ªçµå¼‚ç°è±¡ï¼š

1. Airflow UIä¸ŠåŠ è½½ä¸äº†logï¼Œéœ€è¦åˆ°æœºå™¨ä¸Šæ‰èƒ½çœ‹ï¼›

2. æ˜¾ç¤ºfailçš„taskï¼Œå¹¶æ²¡æœ‰é”™è¯¯æˆ–å¼‚å¸¸ï¼Œåªæ˜¯è¿›ç¨‹è¢«killäº†ï¼Œtask logæ˜¾ç¤º `INFO - Task exited with return code -9`

ä¹Ÿå°±æ˜¯è¯´ï¼ŒAirflowä¸çŸ¥é“å‡ºäºä»€ä¹ˆåŸå› æŠŠæ­£åœ¨è·‘çš„taskç»™killæ‰äº†ã€‚ç»§ç»­æŸ¥ï¼Œå‘ç°æœ‰log`<TaskInstance: *** [running]> detected as zombie`ã€‚çœ‹æ¥å’Œ`zombie` æœ‰å…³ã€‚ç„¶è€Œï¼Œåˆæ˜¯ä»€ä¹ˆåŸå› å¯¼è‡´å¤§é‡zombie tasksçš„äº§ç”Ÿå‘¢ï¼Ÿæœ€åï¼ŒæŸ¥æ‰¾system logï¼Œå‘ç°æ˜¯OOMäº†ï¼Œpythonçš„è¿›ç¨‹ç”±äºOOMè¢«os killäº†ã€‚

äºæ˜¯ï¼Œåˆæ­¥ä¼°è®¡æ˜¯DAGå¹¶è¡Œrunçš„taskå¤ªå¤šï¼Œfolkå‡ºæ¥çš„è¿›ç¨‹å¤ªå¤šï¼Œä½¿workeræ’‘ä¸ä½äº†ã€‚ä¸ºäº†éªŒè¯è¿™ä¸ªçŒœæƒ³ï¼ŒåŠ äº†ä¸€å°workeråå†è·‘ï¼Œæœç„¶ä¸å†æŠ¥é”™äº†ğŸ‘ã€‚çœ‹æ¥å¹¶è¡Œçš„taskå¤šäº†ï¼Œå¯¹airflowæœ¬èº«çš„scheduleä¹Ÿæ˜¯ä¸ªä¸å°çš„è€ƒéªŒã€‚

åªæ˜¯ï¼ŒåŠ workerçš„æ–¹å¼æ€»å½’æ˜¯ä¸ªä¸´æ—¶çš„è¡¥æ•‘æ–¹æ¡ˆï¼Œé‚£ä¹ˆæœ‰æ²¡æœ‰æ›´å¥½çš„åŠæ³•ï¼Œèƒ½å¯¹tasksçš„å¹¶å‘åšä¸€ä¸ªé™åˆ¶ï¼Œå‡è½»workerçš„å‹åŠ›ï¼Œä¸å½±å“å…¶ä»–dagçš„taskså‘¢ï¼Ÿè°ƒç ”åå‘ç°ï¼ŒAirflowä¸­çš„poolèƒ½å®ç°è¿™ä¸ªéœ€æ±‚ã€‚

### å®ç°

ç”¨Poolæ¥é™åˆ¶tasksçš„å¹¶å‘ç›¸å½“ç®€å•ï¼Œåªéœ€è¦å‡ æ­¥æ“ä½œã€‚

#### 1. åœ¨Airflow UI ä¸Šå»ºcustom poolã€‚

Poolçš„UIå…¥å£åœ¨: **Admin->Pools**ã€‚

UIä¸Šå¯ä»¥çœ‹åˆ°ï¼Œä¸ºè®¾ç½®å‰ï¼Œæ•´ä¸ªAirflowåªæœ‰ä¸€ä¸ªdefault poolï¼ŒSlots=128ä¸ªã€‚è¿™é‡ŒSlotçš„æ„æ€å¯ä»¥ç†è§£ä¸ºè¿™ä¸ªpoolé‡Œå¯ä»¥åŒæ—¶è·‘128ä¸ªtaskUIä¸Šå¯ä»¥çœ‹åˆ°ï¼Œä¸ºè®¾ç½®å‰ï¼Œæ•´ä¸ªAirflowåªæœ‰ä¸€ä¸ªdefault poolï¼ŒSlots=128ä¸ªã€‚è¿™é‡ŒSlotçš„æ„æ€å¯ä»¥ç†è§£ä¸ºè¿™ä¸ªpoolé‡Œå¯ä»¥åŒæ—¶è·‘128ä¸ªtasksã€‚

ç‚¹å‡»`create`ï¼Œå»ºç«‹éœ€è¦çš„custom poolï¼Œè®¾ç½®slotæ•°ã€‚ä¸‹é¢åˆ†åˆ«å»ºäº†custom_pool1å’Œcustom_pool2ä¸¤ä¸ªpoolï¼Œslotsè®¾ä¸º4å’Œ1ã€‚

![image-20200630143147636](/img/in-post/Airflow/pool2.png)

#### 2. ä¿®æ”¹DAG code

æœ‰äº†è‡ªå®šä¹‰çš„poolåï¼Œè¦åšçš„å°±æ˜¯åœ¨codeé‡ŒæŒ‡å®šæˆ‘ä»¬è¦çš„poolï¼š

```python
pool_op1 = PythonOperator(
    task_id = "cus_pool_1",
    python_callable = cus_pool1,
    pool = "custom_pool1", #this line is to identify the pool
    dag = dag
)
pool_op2 = PythonOperator(
    task_id = "cus_pool_2",
    python_callable = cus_pool2,
    pool = "custom_pool2", #this line is to identify the pool
    dag = dag
)
```

#### 3. Check

æœ€åï¼Œtriggerå¯¹åº”çš„DAGï¼Œå¯ä»¥çœ‹åˆ°æŒ‡å®šäº†å¯¹åº”custom poolçš„taskæ¶ˆè€—çš„æ˜¯custom poolä¸­çš„slotsã€‚åœ¨å¹¶è¡Œtaskæ•°ç›®å¤§äºå¯ç”¨slotsçš„æƒ…å†µä¸‹ï¼Œtaskä¼šè¿›è¡Œæ’é˜Ÿã€‚

![image-20200630144759262](/img/in-post/Airflow/pool2.png)

### æºç 

Airflowæ˜¯æ€ä¹ˆå®ç°poolæœºåˆ¶çš„å‘¢ï¼Ÿè¿™é‡Œæ¥çœ‹ä¸‹å®ƒçš„æºç å§ã€‚

#### Pool Struct

Poolçš„ç»“æ„å®šä¹‰åœ¨`airflow.models.pool`ï¼Œgitåœ°å€ï¼šhttps://github.com/apache/airflow/blob/master/airflow/models/pool.py

1. class PoolStatsï¼Œæ¯”è¾ƒå¥½ç†è§£æ˜¯ç»Ÿè®¡poolçš„ä½¿ç”¨æƒ…å†µï¼Œæœ‰3ä¸ªç»Ÿè®¡é¡¹ï¼Œåˆ†åˆ«å¯¹åº”slotsæ€»æ•°ï¼Œæ­£åœ¨runningçš„æ•°ç›®ï¼Œqueuedçš„æ•°ç›®ï¼Œå’ŒUIä¸Šé¢æ˜¾ç¤ºç›¸å¯¹åº”ã€‚

```python
class PoolStats(TypedDict):
    """ Dictionary containing Pool Stats """
    total: int
    running: int
    queued: int
```

2. class Poolï¼Œå®šä¹‰äº†ä¸€äº›Poolçš„åŸºæœ¬æ“ä½œã€‚å¯ä»¥çœ‹åˆ°poolçš„ä¿¡æ¯éƒ½æ˜¯å­˜åœ¨DBä¸­`slot_pool` è¿™å¼ è¡¨é‡Œçš„ã€‚

   classä¸­çš„poolå¯¹åº”çš„æ˜¯pool_nameï¼Œslotså¯¹åº”poolä¸­çš„slotæ€»æ•°ï¼Œå¯ä»¥è®¾ä¸º-1ï¼Œè¡¨ç¤ºè¿™ä¸ªpoolä¸é™åˆ¶å¹¶è¡Œçš„ä»»åŠ¡ï¼ˆå½“ç„¶ä¸æ¨èè¿™ä¹ˆåšï¼‰ã€‚

```python
class Pool(Base):
    """
    the class to get Pool info.
    """
    __tablename__ = "slot_pool"
    
    id = Column(Integer, primary_key=True)
    pool = Column(String(256), unique=True)
    # -1 for infinite
    slots = Column(Integer, default=0)
    description = Column(Text)

    DEFAULT_POOL_NAME = 'default_pool'


```

3. Class Poolä¸­æœ‰3ä¸ª**staticmethod**ï¼Œéƒ½æ˜¯DB queryçš„æ“ä½œã€‚

   å‰ä¸¤ä¸ª`get_pool`æ˜¯æ ¹æ®pool_nameå»æ‹¿åˆ°poolçš„ç»“æ„ã€‚è¿™é‡Œï¼Œå¤„ç†DBä¸­pool_nameé‡åçš„æƒ…å†µæ˜¯ï¼Œè¿”å›çš„æ˜¯æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªpoolã€‚åœ¨UIä¸Šè¯•äº†ä¸‹createæ“ä½œï¼Œå¹¶ä¸èƒ½åˆ›å»ºé‡åçš„poolï¼Œä¼šæŠ¥`Already exists.`çš„é”™ã€‚çœ‹æ¥è¿™ä¸ªæ˜¯é¢„é˜²ç›´æ¥å†™å…¥DBæ—¶å‡ºç°çš„é‡åé—®é¢˜ã€‚

   `slots_stats`é‡Œimportäº†`TaskInstance`ï¼Œä¼šæ ¹æ®å½“å‰task runçš„æƒ…å†µè¿”å›æ‰€æœ‰poolçš„çŠ¶æ€ï¼Œä¸Šé¢çš„UIæ˜¾ç¤ºå°±æ˜¯è°ƒç”¨çš„è¿™ä¸ªæ¥å£ã€‚

```python
def get_pool(pool_name, session: Session = None):
  return session.query(Pool).filter(Pool.pool == pool_name).first()

def get_default_pool(session: Session = None):
  return Pool.get_pool(Pool.DEFAULT_POOL_NAME, session=session)

def slots_stats(session: Session = None) -> Dict[str, PoolStats]:
  #å…³é”®æ˜¯è¿™å‡ è¡Œ
  #æ‹¿åˆ°æ‰€æœ‰çš„pools
  pool_rows: Iterable[Tuple[str, int]] = session.query(Pool.pool, Pool.slots).all()
  #åˆå§‹åŒ–è¿”å›çš„PoolStats
  for (pool_name, total_slots) in pool_rows:
  		pools[pool_name] = PoolStats(total=total_slots, running=0, queued=0)
	#æŸ¥è¯¢taskå ç”¨poolçš„æƒ…å†µï¼ŒEXECUTION_STATESå®šä¹‰åœ¨airflow.ti_deps.dependencies_states,å°±æ˜¯runningå’Œqueuedçš„çŠ¶æ€å’Œ
  '''
    EXECUTION_STATES = {
      State.RUNNING,
      State.QUEUED,
  	}
  '''
  state_count_by_pool = (
  		session.query(TaskInstance.pool, TaskInstance.state, func.count())
      .filter(TaskInstance.state.in_(list(EXECUTION_STATES)))
      .group_by(TaskInstance.pool, TaskInstance.state)
  ).all()
  #ç„¶åå°±æ‹¼æˆç»“æœè¿”å›äº†
```

4. class Poolä¸­è¿˜æœ‰å‡ ä¸ªæ–¹æ³•ï¼Œæ˜¯è·å–ä¸åŒçŠ¶æ€çš„slotsæ•°ç›®ã€‚

   codeæ¯”è¾ƒç®€å•ï¼Œéƒ½æ˜¯ä»DB queryåæ ¹æ®çŠ¶æ€åšçš„filterï¼Œè´´ä¸€ä¸ª`occupied_slots` çš„å®ç°ï¼Œå…¶ä»–çš„éƒ½ç±»ä¼¼ã€‚å…¶ä¸­running_slotså’Œqueued_slotsæ¯”è¾ƒå¥½ç†è§£ï¼Œoccupied_slots=running_slots+queued_slotsï¼Œopen_slots=Pool.slots-occupied_slotsã€‚

```python
def occupied_slots(self, session: Session):
  from airflow.models.taskinstance import TaskInstance  # Avoid circular import
  return (
  		session
      .query(func.sum(TaskInstance.pool_slots))
      .filter(TaskInstance.pool == self.pool)
      .filter(TaskInstance.state.in_(list(EXECUTION_STATES)))
      .scalar()
  ) or 0
def running_slots(self, session: Session):
def queued_slots(self, session: Session):
def open_slots(self, session: Session):
```

5. æ­¤å¤–ï¼Œè¿˜æä¾›äº†`to_json`æ–¹æ³•ï¼Œä»¥jsonæ ¼å¼è¿”å›Poolçš„ç»“æ„ã€‚

```python
def to_json(self):
  return {
    'id': self.id,
    'pool': self.pool,
    'slots': self.slots,
    'description': self.description,
  }
```

#### Usage in Operator

æ¥ä¸‹æ¥ï¼Œçœ‹ä¸‹operatoræ˜¯å¦‚ä½•ä¼ é€’è¿™ä¸ªpoolçš„ã€‚åŸºæœ¬çš„å®ç°åœ¨`BaseOperator`ä¸­ï¼Œgit link: https://github.com/apache/airflow/blob/master/airflow/models/baseoperator.py

çœ‹å‚æ•°å®šä¹‰ï¼Œé™¤äº†æ”¯æŒpoolæŒ‡å®špool_nameï¼Œè¿˜æ”¯æŒpool_slotsï¼Œé»˜è®¤å€¼æ˜¯1ï¼Œå¿…é¡»å¤§äºç­‰äº1ï¼Œè¡¨ç¤ºçš„æ˜¯è¿™ä¸ªtaskéœ€è¦å ç”¨çš„slotsæ•°ç›®ã€‚é™¤äº†æŒ‡å®šè¿™ä¸¤ä¸ªå€¼ï¼Œoperatorä¸­æ²¡æœ‰å¯¹poolçš„é¢å¤–æ“ä½œã€‚

```python
'''
    :param pool: the slot pool this task should run in, slot pools are a
    way to limit concurrency for certain tasks
    :type pool: str
    :param pool_slots: the number of pool slots this task should use (>= 1)
        Values less than 1 are not allowed.
    :type pool_slots: int
'''
def __init__(
    ...
    pool: Optional[str] = None,
    pool_slots: int = 1,
    ...
):
    self.pool = Pool.DEFAULT_POOL_NAME if pool is None else pool
    self.pool_slots = pool_slots
    if self.pool_slots < 1:
    	raise AirflowException("pool slots for %s in dag %s cannot be less than 1"
      		% (self.task_id, dag.dag_id))
```

#### Implementation in job scheduler

å®šä¹‰äº†poolç»“æ„ï¼Œå¹¶æŒ‡å®šäº†taskç”¨çš„poolåï¼Œå†æ¥çœ‹ä¸‹airflowæ˜¯æ€ä¹ˆåŸºäºpoolè¿›è¡Œjobsè°ƒåº¦çš„ã€‚å®ç°çš„codeåœ¨`class SchedulerJob`ä¸­ï¼Œgit link: https://github.com/apache/airflow/blob/master/airflow/jobs/scheduler_job.py å…³é”®çš„codeå·²ç»æ‘˜å‡ºæ¥äº†ï¼š

```python
def _find_executable_task_instances(self, simple_dag_bag: SimpleDagBag, session=None):
  # Get all task instances associated with scheduled
  # DagRuns which are not backfilled, in the given states,
  # and the dag is not paused
  #è¿™ä¸ªæ˜¯æŒ‰ä¸€å®šæ¡ä»¶æ‰¾åˆ°å¯ä»¥executeçš„task_instancesï¼Œéšä¾¿çœ‹çœ‹å§
  task_instances_to_examine = (
    session
    .query(TI)
    .filter(TI.dag_id.in_(simple_dag_bag.dag_ids))
    .outerjoin(
      DR, and_(DR.dag_id == TI.dag_id, DR.execution_date == TI.execution_date)
    )
    .filter(or_(DR.run_id.is_(None), DR.run_type != DagRunType.BACKFILL_JOB.value))
    .outerjoin(DM, DM.dag_id == TI.dag_id)
    .filter(or_(DM.dag_id.is_(None), not_(DM.is_paused)))
    .filter(TI.state == State.SCHEDULED)
    .all()
  )
  # Get the pool settings
  pools = {p.pool: p for p in session.query(models.Pool).all()}
  #æŠŠtask_instances_to_examineæ”¾åˆ°ä»¥pool_nameä¸ºç´¢å¼•çš„dicté‡Œï¼Œç»“æ„æ˜¯pool_name: task_instance list
  pool_to_task_instances = defaultdict(list)
  for task_instance in task_instances_to_examine:
    pool_to_task_instances[task_instance.pool].append(task_instance)

  # Go through each pool, and queue up a task for execution if there are
  # any open slots in the pool.
  # pylint: disable=too-many-nested-blocks
  #æŒ¨ä¸ªçœ‹pool_to_task_instancesé‡Œçš„pool
  for pool, task_instances in pool_to_task_instances.items():
  	pool_name = pool
    #æ„æ€æ˜¯å¦‚æœæŒ‡å®šçš„poolåœ¨airflowé‡Œæ²¡æœ‰çš„è¯ï¼Œè¿™ä¸ªtaskå°±ä¸ä¼šè¢«æ‰§è¡Œã€‚ä½†åªæœ‰ä¸€ä¸ªwarnningçš„logï¼Œæ¯”è¾ƒéš¾å‘ç°ï¼Œåªèƒ½é å†™çš„æ—¶å€™æ³¨æ„äº†ã€‚
    if pool not in pools:
    	self.log.warning(
        "Tasks using non-existent pool '%s' will not be scheduled",
        pool
      )
      continue
    #æ‰¾åˆ°poolé‡Œçš„open_slotsï¼Œè¿˜è®°å¾—open_slotsçš„å®šä¹‰ä¸ï¼Ÿ
    open_slots = pools[pool].open_slots(session=session)
    num_ready = len(task_instances)
    #è¿™æ˜¯å¯¹åŒä¸€ä¸ªpoolé‡Œå¯æ‰§è¡Œçš„task_instancesæŒ‰æƒé‡æ’åºï¼Œå…¶ä¸­priority_weightä¹Ÿå¯ä»¥åœ¨operatorä¸­æŒ‡å®šã€‚æ’åºæ˜¯åŸºäºpriority_weightï¼ˆå€’åºï¼‰å’Œtaskçš„æ‰§è¡Œæ—¶é—´ï¼ˆé¡ºåºï¼‰ã€‚
    priority_sorted_task_instances = sorted(
    	task_instances, key=lambda ti: (-ti.priority_weight, ti.execution_date))
    #ä¸ºæ’å®Œåºçš„tasksåˆ†é…slots
    for current_index, task_instance in enumerate(priority_sorted_task_instances):
      #å…ˆæ£€æŸ¥åˆ†é…å®Œçš„æƒ…å†µï¼Œå°±æ˜¯æ‰“logï¼Œç»Ÿè®¡åˆ†é…çš„ä¸ªæ•°ç­‰ç­‰ã€‚ã€‚
      if open_slots <= 0:
      	self.log.info(
          "Not scheduling since there are %s open slots in pool %s",
          open_slots, pool
        )
        # Can't schedule any more since there are no more open slots.
        num_unhandled = len(priority_sorted_task_instances) - current_index
        num_starving_tasks += num_unhandled
        num_starving_tasks_total += num_unhandled
        break
        
      # è¦å…ˆæ£€æŸ¥dag levelçš„task concurrency
      # Check to make sure that the task concurrency of the DAG hasn't been
      # reached.
      simple_dag = simple_dag_bag.get_dag(dag_id)
      #dagä¸Šå¯ä»¥è®¾ç½®task cuncurrency(dag_concurrency_limit)ï¼Œè¡¨ç¤ºè¿™ä¸ªdagæœ€å¤šèƒ½å¹¶è¡Œè·‘å¤šå°‘ä¸ªtasks
      current_dag_concurrency = dag_concurrency_map[dag_id]
      dag_concurrency_limit = simple_dag_bag.get_dag(dag_id).concurrency
      if current_dag_concurrency >= dag_concurrency_limit:
        continue
      #taskä¸Šå¯ä»¥è®¾ç½®task cuncurrency(task_concurrency_limit)ï¼Œè¿™æ˜¯åŒä¸€ä¸ªDAGåŒæ—¶åœ¨è·‘å¤šä»½jobçš„æƒ…å†µä¸‹ï¼Œé™åˆ¶ä¸‹DAGä¸­æŒ‡å®štaskçš„å¹¶å‘ã€‚
      task_concurrency_limit = simple_dag.get_task_special_arg(
        task_instance.task_id,
        'task_concurrency')
      if task_concurrency_limit is not None:
        current_task_concurrency = task_concurrency_map[
          (task_instance.dag_id, task_instance.task_id)
        ]
        if current_task_concurrency >= task_concurrency_limit:
          continue
      
      #å¦‚æœtaskå·²ç»è·‘ä¸Šäº†ï¼Œå°±ä¸å†scheduleäº†(ä¸è¿‡ä¸å¤ªæ¸…æ¥šä»€ä¹ˆæ—¶å€™ä¼šå‡ºç°è¿™ä¸ªæƒ…å†µ)
      if self.executor.has_task(task_instance):
        self.log.debug(
          "Not handling task %s as the executor reports it is running",
          task_instance.key
        )
        num_tasks_in_executor += 1
        continue
      #operatoré‡ŒæŒ‡å®šçš„pool_slotsï¼Œå¦‚æœå¤§äºopen_slotsï¼Œtaskå°±ç»§ç»­ç­‰ç€
    	if task_instance.pool_slots > open_slots:
        self.log.info("Not executing %s since it requires %s slots "
                      "but there are %s open slots in the pool %s.",
                      task_instance, task_instance.pool_slots, open_slots, pool)
        num_starving_tasks += 1
        num_starving_tasks_total += 1
        # Though we can execute tasks with lower priority if there's enough room
        continue
      #ç»è¿‡ä¸€ç³»åˆ—åˆ¤æ–­ï¼Œåˆ°è¿™ä¸€æ­¥çš„taskç»ˆäºè¿›å…¥åˆ°runningçš„çŠ¶æ€äº†ï¼ï¼ï¼
      executable_tis.append(task_instance)
      open_slots -= task_instance.pool_slots
      dag_concurrency_map[dag_id] += 1
      task_concurrency_map[(task_instance.dag_id, task_instance.task_id)] += 1
```

çœ‹äº†ä¸Šé¢çš„æºç ï¼Œå¯ä»¥å‘ç°é™¤äº†æŒ‡å®šPoolï¼Œè¿˜å¯ä»¥ï¼š

1. é€šè¿‡è®¾ç½®DAGçš„`task concurrency`æ¥é™åˆ¶taskå¹¶å‘
2. è®¾ç½®taskçš„`task concurrency`æ¥é™åˆ¶DAGå¹¶è¡Œrunæƒ…å†µä¸‹çš„æŸä¸ªtaskçš„å¹¶å‘
3. é€šè¿‡`priority_weight`å½±å“taskè°ƒåº¦çš„ä¼˜å…ˆçº§
4. ä»¥ä¸Šå¯ä»¥å’Œpoolå…±ç”¨

### æ€»ç»“

é€šè¿‡Airflow Poolï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿çš„é™åˆ¶tasksçš„å¹¶è¡Œåº¦ã€‚å…¶å®ï¼ŒAirflowè¿˜æä¾›äº†queueçš„åŠŸèƒ½ï¼Œä¹Ÿå¯ä»¥å®ç°ç±»ä¼¼çš„åŠŸèƒ½ã€‚è€Œå…·ä½“queueå’Œpoolçš„åŒºåˆ«åœ¨å“ªï¼Œéœ€è¦ç»§ç»­è°ƒç ”ã€‚



